from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from ..utils.helpers import count_tokens


@dataclass
class MarkdownChunk:
    """Markdownæ–‡æ¡£å—"""
    id: str
    content: str
    metadata: Dict[str, Any]
    token_count: int
    headers: List[tuple]  # [(level, title), ...]


class MarkdownDocumentParser:
    """åŸºäºLangchainçš„Markdownæ–‡æ¡£è§£æå™¨"""
    
    def __init__(self, chunk_size: int = 3000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # å®šä¹‰æ ‡é¢˜åˆ†å‰²å™¨çš„å±‚çº§
        self.headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"), 
            ("###", "Header 3"),
            ("####", "Header 4"),
            ("#####", "Header 5"),
            ("######", "Header 6"),
        ]
        
        # åˆ›å»ºmarkdownæ ‡é¢˜åˆ†å‰²å™¨
        self.markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=self.headers_to_split_on,
            strip_headers=False
        )
        
        # åˆ›å»ºå­—ç¬¦åˆ†å‰²å™¨ï¼ˆç”¨äºå¤„ç†è¿‡é•¿çš„å—ï¼‰
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=count_tokens,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def parse_markdown_file(self, file_path: Path) -> List[MarkdownChunk]:
        """è§£æMarkdownæ–‡ä»¶"""
        
        # è¯»å–markdownå†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        
        # æ¸…ç†å†…å®¹
        markdown_content = self._clean_markdown(markdown_content)
        
        # ç¬¬ä¸€æ­¥ï¼šæŒ‰æ ‡é¢˜åˆ†å‰²
        header_chunks = self.markdown_splitter.split_text(markdown_content)
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†è¿‡é•¿çš„å—
        final_chunks = []
        for i, chunk in enumerate(header_chunks):
            token_count = count_tokens(chunk.page_content)
            
            if token_count <= self.chunk_size:
                # å—å¤§å°åˆé€‚ï¼Œç›´æ¥ä½¿ç”¨
                markdown_chunk = MarkdownChunk(
                    id=f"chunk_{i}",
                    content=chunk.page_content,
                    metadata=chunk.metadata,
                    token_count=token_count,
                    headers=self._extract_headers(chunk.metadata)
                )
                final_chunks.append(markdown_chunk)
            else:
                # å—å¤ªå¤§ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                sub_chunks = self.text_splitter.split_text(chunk.page_content)
                for j, sub_chunk in enumerate(sub_chunks):
                    sub_token_count = count_tokens(sub_chunk)
                    markdown_chunk = MarkdownChunk(
                        id=f"chunk_{i}_{j}",
                        content=sub_chunk,
                        metadata=chunk.metadata,
                        token_count=sub_token_count,
                        headers=self._extract_headers(chunk.metadata)
                    )
                    final_chunks.append(markdown_chunk)
        
        return final_chunks
    
    def _clean_markdown(self, content: str) -> str:
        """æ¸…ç†markdownå†…å®¹"""
        lines = content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # è·³è¿‡ç©ºè¡Œå¤ªå¤šçš„æƒ…å†µ
            if line.strip():
                cleaned_lines.append(line)
            elif not cleaned_lines or cleaned_lines[-1].strip():
                # ä¿ç•™å¿…è¦çš„ç©ºè¡Œ
                cleaned_lines.append('')
        
        return '\n'.join(cleaned_lines)
    
    def _extract_headers(self, metadata: Dict[str, Any]) -> List[tuple]:
        """ä»metadataä¸­æå–æ ‡é¢˜å±‚çº§ä¿¡æ¯"""
        headers = []
        for key, value in metadata.items():
            if key.startswith('Header ') and value:
                level = int(key.split(' ')[1])
                headers.append((level, value))
        
        # æŒ‰å±‚çº§æ’åº
        headers.sort(key=lambda x: x[0])
        return headers
    
    def get_chunk_hierarchy(self, chunks: List[MarkdownChunk]) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£å—çš„å±‚æ¬¡ç»“æ„"""
        hierarchy = {
            'total_chunks': len(chunks),
            'total_tokens': sum(chunk.token_count for chunk in chunks),
            'sections': {},
            'chunk_distribution': {}
        }
        
        # ç»Ÿè®¡å„ä¸ªæ ‡é¢˜ä¸‹çš„å—æ•°é‡
        for chunk in chunks:
            if chunk.headers:
                # ä½¿ç”¨æœ€é«˜çº§åˆ«çš„æ ‡é¢˜ä½œä¸ºä¸»åˆ†ç±»
                main_header = chunk.headers[0][1] if chunk.headers else 'Unknown'
                if main_header not in hierarchy['sections']:
                    hierarchy['sections'][main_header] = {
                        'chunks': 0,
                        'tokens': 0,
                        'level': chunk.headers[0][0] if chunk.headers else 0
                    }
                
                hierarchy['sections'][main_header]['chunks'] += 1
                hierarchy['sections'][main_header]['tokens'] += chunk.token_count
        
        # ç»Ÿè®¡tokenåˆ†å¸ƒ
        token_ranges = [(0, 500), (500, 1000), (1000, 2000), (2000, 3000), (3000, float('inf'))]
        for start, end in token_ranges:
            range_key = f"{start}-{end if end != float('inf') else 'inf'}"
            count = len([c for c in chunks if start <= c.token_count < end])
            hierarchy['chunk_distribution'][range_key] = count
        
        return hierarchy
    
    def print_hierarchy(self, chunks: List[MarkdownChunk]) -> None:
        """æ‰“å°æ–‡æ¡£å±‚æ¬¡ç»“æ„"""
        hierarchy = self.get_chunk_hierarchy(chunks)
        
        print("ğŸ“Š æ–‡æ¡£ç»“æ„åˆ†æ:")
        print(f"   æ€»å—æ•°: {hierarchy['total_chunks']}")
        print(f"   æ€»tokenæ•°: {hierarchy['total_tokens']:,}")
        
        print("\nğŸ“š ç« èŠ‚åˆ†å¸ƒ:")
        sections = sorted(hierarchy['sections'].items(), 
                         key=lambda x: x[1]['level'])
        for section, info in sections:
            level_prefix = "  " * (info['level'] - 1)
            print(f"   {level_prefix}â€¢ {section}: {info['chunks']} å—, {info['tokens']:,} tokens")
        
        print("\nğŸ“ˆ Tokenåˆ†å¸ƒ:")  
        for range_key, count in hierarchy['chunk_distribution'].items():
            print(f"   {range_key} tokens: {count} å—")


def main():
    """æµ‹è¯•å‡½æ•°"""
    parser = MarkdownDocumentParser(chunk_size=2000, chunk_overlap=100)
    
    # æµ‹è¯•è§£æGFM_SURVEYçš„full.md
    md_file = Path("../GFM_SURVEY/full.md")
    if md_file.exists():
        print(f"è§£ææ–‡ä»¶: {md_file}")
        chunks = parser.parse_markdown_file(md_file)
        
        parser.print_hierarchy(chunks)
        
        print(f"\nå‰3ä¸ªå—çš„å†…å®¹é¢„è§ˆ:")
        for i, chunk in enumerate(chunks[:3]):
            print(f"\nå— {i+1} (ID: {chunk.id}):")
            print(f"   Headers: {chunk.headers}")
            print(f"   Tokens: {chunk.token_count}")
            print(f"   Content preview: {chunk.content[:200]}...")
    else:
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {md_file}")


if __name__ == "__main__":
    main()