import json
import pickle
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib

from .document_parser import DocumentChunk


@dataclass
class SummaryResult:
    """æ€»ç»“ç»“æœ"""

    chunk_id: str
    original_content: str
    summary: str
    section_title: str
    section_level: int
    token_count: int
    compression_ratio: float
    metadata: Dict[str, Any]


@dataclass
class CheckpointState:
    """æ£€æŸ¥ç‚¹çŠ¶æ€"""

    session_id: str
    input_dir: str
    output_path: str
    compression_level: int

    # æ‰§è¡ŒçŠ¶æ€
    total_chapters: int
    processed_chapters: int
    current_stage: str  # 'initial_summary' | 'polish' | 'completed'

    # æ—¶é—´æˆ³
    created_at: str
    updated_at: str

    # æ•°æ®æ–‡ä»¶è·¯å¾„
    chapters_file: str
    summaries_file: str
    polished_summaries_file: Optional[str]

    # å®ŒæˆçŠ¶æ€
    completed_chapter_ids: List[str]
    failed_chapter_ids: List[str]
    error_log: List[Dict[str, str]]


class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""

    def __init__(self, checkpoint_dir: Path = None):
        self.checkpoint_dir = checkpoint_dir or Path(".checkpoints")
        self.checkpoint_dir.mkdir(exist_ok=True)

    def create_session_id(self, input_dir: Path, compression_level: int) -> str:
        """åˆ›å»ºä¼šè¯ID"""
        # åŸºäºè¾“å…¥ç›®å½•å’Œå‚æ•°ç”Ÿæˆå”¯ä¸€ID
        content = f"{input_dir.absolute()}_{compression_level}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return hashlib.md5(content.encode()).hexdigest()[:12]

    def save_checkpoint(
        self,
        state: CheckpointState,
        chapters: List[DocumentChunk] = None,
        summaries: List[SummaryResult] = None,
        polished_summaries: List[SummaryResult] = None,
    ) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        session_dir = self.checkpoint_dir / state.session_id
        session_dir.mkdir(exist_ok=True)

        # æ›´æ–°æ—¶é—´æˆ³
        state.updated_at = datetime.now().isoformat()

        # ä¿å­˜çŠ¶æ€æ–‡ä»¶
        state_file = session_dir / "state.json"
        with open(state_file, "w", encoding="utf-8") as f:
            json.dump(asdict(state), f, indent=2, ensure_ascii=False)

        # ä¿å­˜æ•°æ®æ–‡ä»¶
        if chapters is not None:
            chapters_file = session_dir / "chapters.pkl"
            with open(chapters_file, "wb") as f:
                pickle.dump(chapters, f)
            state.chapters_file = str(chapters_file)

        if summaries is not None:
            summaries_file = session_dir / "summaries.pkl"
            with open(summaries_file, "wb") as f:
                pickle.dump(summaries, f)
            state.summaries_file = str(summaries_file)

        if polished_summaries is not None:
            polished_file = session_dir / "polished_summaries.pkl"
            with open(polished_file, "wb") as f:
                pickle.dump(polished_summaries, f)
            state.polished_summaries_file = str(polished_file)

        # å†æ¬¡ä¿å­˜çŠ¶æ€ï¼ˆæ›´æ–°æ–‡ä»¶è·¯å¾„ï¼‰
        with open(state_file, "w", encoding="utf-8") as f:
            json.dump(asdict(state), f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {state.session_id}")
        print(f"   å·²å¤„ç†: {state.processed_chapters}/{state.total_chapters} ç« èŠ‚")
        print(f"   å½“å‰é˜¶æ®µ: {state.current_stage}")
        print(f"   å¤±è´¥æ•°é‡: {len(state.failed_chapter_ids)}")

    def load_checkpoint(
        self, session_id: str
    ) -> tuple[
        CheckpointState,
        List[DocumentChunk],
        List[SummaryResult],
        Optional[List[SummaryResult]],
    ]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        session_dir = self.checkpoint_dir / session_id
        if not session_dir.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {session_id}")

        # åŠ è½½çŠ¶æ€
        state_file = session_dir / "state.json"
        with open(state_file, "r", encoding="utf-8") as f:
            state_dict = json.load(f)

        state = CheckpointState(**state_dict)

        # åŠ è½½æ•°æ®
        chapters = []
        if state.chapters_file and Path(state.chapters_file).exists():
            with open(state.chapters_file, "rb") as f:
                chapters = pickle.load(f)

        summaries = []
        if state.summaries_file and Path(state.summaries_file).exists():
            with open(state.summaries_file, "rb") as f:
                summaries = pickle.load(f)

        polished_summaries = None
        if (
            state.polished_summaries_file
            and Path(state.polished_summaries_file).exists()
        ):
            with open(state.polished_summaries_file, "rb") as f:
                polished_summaries = pickle.load(f)

        print(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {session_id}")
        print(f"   åˆ›å»ºæ—¶é—´: {state.created_at}")
        print(f"   æ›´æ–°æ—¶é—´: {state.updated_at}")
        print(f"   å½“å‰é˜¶æ®µ: {state.current_stage}")
        print(f"   è¿›åº¦: {state.processed_chapters}/{state.total_chapters}")

        return state, chapters, summaries, polished_summaries

    def list_checkpoints(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹"""
        checkpoints = []

        for session_dir in self.checkpoint_dir.iterdir():
            if session_dir.is_dir():
                state_file = session_dir / "state.json"
                if state_file.exists():
                    try:
                        with open(state_file, "r", encoding="utf-8") as f:
                            state_dict = json.load(f)

                        checkpoints.append(
                            {
                                "session_id": state_dict["session_id"],
                                "input_dir": state_dict["input_dir"],
                                "compression_level": state_dict["compression_level"],
                                "current_stage": state_dict["current_stage"],
                                "progress": f"{state_dict['processed_chapters']}/{state_dict['total_chapters']}",
                                "created_at": state_dict["created_at"],
                                "updated_at": state_dict["updated_at"],
                                "failed_count": len(
                                    state_dict.get("failed_chapter_ids", [])
                                ),
                            }
                        )
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•è¯»å–æ£€æŸ¥ç‚¹ {session_dir.name}: {e}")

        # æŒ‰æ›´æ–°æ—¶é—´æ’åº
        checkpoints.sort(key=lambda x: x["updated_at"], reverse=True)
        return checkpoints

    def clean_old_checkpoints(self, keep_days: int = 7) -> None:
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        cutoff_time = datetime.now().timestamp() - (keep_days * 24 * 3600)
        cleaned_count = 0

        for session_dir in self.checkpoint_dir.iterdir():
            if session_dir.is_dir():
                state_file = session_dir / "state.json"
                if state_file.exists():
                    try:
                        with open(state_file, "r", encoding="utf-8") as f:
                            state_dict = json.load(f)

                        created_time = datetime.fromisoformat(
                            state_dict["created_at"]
                        ).timestamp()
                        if created_time < cutoff_time:
                            # åˆ é™¤æ•´ä¸ªä¼šè¯ç›®å½•
                            import shutil

                            shutil.rmtree(session_dir)
                            cleaned_count += 1
                            print(f"ğŸ—‘ï¸ å·²æ¸…ç†æ£€æŸ¥ç‚¹: {session_dir.name}")
                    except Exception as e:
                        print(f"âš ï¸ æ¸…ç†æ£€æŸ¥ç‚¹å¤±è´¥ {session_dir.name}: {e}")

        if cleaned_count > 0:
            print(f"âœ… å·²æ¸…ç† {cleaned_count} ä¸ªè¿‡æœŸæ£€æŸ¥ç‚¹")
        else:
            print("â„¹ï¸ æ²¡æœ‰è¿‡æœŸçš„æ£€æŸ¥ç‚¹éœ€è¦æ¸…ç†")

    def delete_checkpoint(self, session_id: str) -> bool:
        """åˆ é™¤æŒ‡å®šæ£€æŸ¥ç‚¹"""
        session_dir = self.checkpoint_dir / session_id
        if session_dir.exists():
            try:
                import shutil

                shutil.rmtree(session_dir)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ£€æŸ¥ç‚¹: {session_id}")
                return True
            except Exception as e:
                print(f"âŒ åˆ é™¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                return False
        else:
            print(f"âš ï¸ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {session_id}")
            return False


def print_checkpoints_table(checkpoints: List[Dict[str, Any]]) -> None:
    """æ‰“å°æ£€æŸ¥ç‚¹è¡¨æ ¼"""
    if not checkpoints:
        print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
        return

    print(f"\nğŸ“‹ å‘ç° {len(checkpoints)} ä¸ªæ£€æŸ¥ç‚¹:")
    print("=" * 120)
    print(
        f"{'ä¼šè¯ID':<12} {'è¾“å…¥ç›®å½•':<25} {'å‹ç¼©':<4} {'é˜¶æ®µ':<15} {'è¿›åº¦':<10} {'å¤±è´¥':<4} {'æ›´æ–°æ—¶é—´':<19}"
    )
    print("-" * 120)

    for cp in checkpoints:
        print(
            f"{cp['session_id']:<12} {cp['input_dir'][-24:]:<25} "
            f"{cp['compression_level']:<4}% {cp['current_stage']:<15} "
            f"{cp['progress']:<10} {cp['failed_count']:<4} {cp['updated_at'][:19]}"
        )

    print("=" * 120)
