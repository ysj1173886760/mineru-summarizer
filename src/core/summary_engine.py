import asyncio
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
from tqdm import tqdm

from ..config.unified_config import UnifiedConfig
from .document_parser import DocumentParser, DocumentChunk
from .backend_manager import BackendManager, BackendProtocol
from .checkpoint_manager import (
    CheckpointManager,
    CheckpointState,
    SummaryResult,
    print_checkpoints_table,
)
from ..output.markdown_formatter_v2 import MarkdownFormatterV2


class SummaryEngine:
    """ç»Ÿä¸€çš„æ€»ç»“å¼•æ“"""

    def __init__(self, config: UnifiedConfig):
        self.config = config
        self.checkpoint_manager = (
            CheckpointManager(Path(config.processing.checkpoint_dir))
            if config.processing.enable_checkpoint
            else None
        )

    async def summarize(
        self,
        input_dir: Path,
        output_path: Path,
        compression_level: int = 50,
        resume_session: Optional[str] = None,
        debug_pairs: bool = False,
    ) -> None:
        """æ‰§è¡Œæ€»ç»“ä»»åŠ¡"""

        if resume_session:
            await self._resume_summarize(resume_session, debug_pairs)
        else:
            await self._new_summarize(
                input_dir, output_path, compression_level, debug_pairs
            )

    async def _new_summarize(
        self,
        input_dir: Path,
        output_path: Path,
        compression_level: int,
        debug_pairs: bool = False,
    ) -> None:
        """æ–°çš„æ€»ç»“ä»»åŠ¡"""

        print(f"ğŸš€ å¼€å§‹å¤„ç†MinerUæ•°æ® (ç»Ÿä¸€ç‰ˆæœ¬): {input_dir}")
        print(f"ğŸ“Š å‹ç¼©çº§åˆ«: {compression_level}%")
        print(f"ğŸ“„ è¾“å‡ºè·¯å¾„: {output_path}")
        print(f"ğŸ“š æœ€å¤§ç« èŠ‚tokenæ•°: {self.config.processing.max_tokens_per_chapter:,}")
        print(f"ğŸ¤– åç«¯: {self.config.backend.type}")
        print(f"âœ¨ äºŒæ¬¡æ‰“ç£¨: {'å¯ç”¨' if self.config.polish.enabled else 'ç¦ç”¨'}")
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹: {'å¯ç”¨' if self.config.processing.enable_checkpoint else 'ç¦ç”¨'}")
        if debug_pairs:
            print(f"ğŸ” Debugæ¨¡å¼: å°†ä¿å­˜æ‰€æœ‰(åŸæ–‡,æ€»ç»“)å¯¹åˆ° {output_path.parent / 'debug_pairs'}")

        # 1. è§£æMarkdownæ–‡æ¡£
        print("\næ­¥éª¤1: æŒ‰å¤§ç« èŠ‚è§£æMarkdownæ–‡æ¡£...")
        full_md_path = input_dir / "full.md"
        if not full_md_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°full.mdæ–‡ä»¶: {full_md_path}")

        parser = DocumentParser(self.config.processing.max_tokens_per_chapter)
        chapter_chunks = parser.parse_markdown_file(full_md_path)

        print(f"è§£æå®Œæˆ: {len(chapter_chunks)} ä¸ªå¤§ç« èŠ‚")
        parser.print_statistics(chapter_chunks)

        # 2. åˆ›å»ºæ£€æŸ¥ç‚¹çŠ¶æ€ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        state = None
        if self.checkpoint_manager:
            session_id = self.checkpoint_manager.create_session_id(
                input_dir, compression_level
            )
            state = CheckpointState(
                session_id=session_id,
                input_dir=str(input_dir.absolute()),
                output_path=str(output_path.absolute()),
                compression_level=compression_level,
                total_chapters=len(chapter_chunks),
                processed_chapters=0,
                current_stage="initial_summary",
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
                chapters_file="",
                summaries_file="",
                polished_summaries_file=None,
                completed_chapter_ids=[],
                failed_chapter_ids=[],
                error_log=[],
            )

            # ä¿å­˜åˆå§‹æ£€æŸ¥ç‚¹
            self.checkpoint_manager.save_checkpoint(state, chapters=chapter_chunks)
            print(f"ğŸ’¾ å·²åˆ›å»ºä¼šè¯: {session_id}")

        # 3. ç”Ÿæˆåˆæ­¥æ€»ç»“
        print(f"\næ­¥éª¤2: ç”Ÿæˆåˆæ­¥æ€»ç»“...")
        summaries = await self._generate_summaries_with_checkpoint(
            chapter_chunks,
            compression_level,
            state,
            debug_pairs=debug_pairs,
            output_path=output_path,
        )
        print(f"åˆæ­¥æ€»ç»“ç”Ÿæˆå®Œæˆ: {len(summaries)} ä¸ªæ€»ç»“å—")

        # 4. äºŒæ¬¡æ‰“ç£¨ï¼ˆå¯é€‰ï¼‰
        final_summaries = summaries
        if self.config.polish.enabled and summaries:
            print(f"\næ­¥éª¤3: äºŒæ¬¡æ‰“ç£¨æ€»ç»“...")
            if state:
                state.current_stage = "polish"
                state.processed_chapters = 0
                state.completed_chapter_ids = []
                self.checkpoint_manager.save_checkpoint(state, summaries=summaries)

            polished_summaries = await self._polish_summaries_with_checkpoint(
                summaries,
                compression_level,
                state,
                debug_pairs=debug_pairs,
                output_path=output_path,
            )
            print(f"æ‰“ç£¨å®Œæˆ: {len(polished_summaries)} ä¸ªç²¾åŒ–æ€»ç»“")
            final_summaries = polished_summaries

        # 5. å®Œæˆå¤„ç†
        await self._finalize_summary(
            final_summaries, state, chapter_chunks, output_path, compression_level
        )

        # ç”Ÿæˆdebugç´¢å¼•æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if debug_pairs:
            await self._create_debug_index(output_path, final_summaries)

    async def _resume_summarize(
        self, session_id: str, debug_pairs: bool = False
    ) -> None:
        """æ¢å¤æ€»ç»“ä»»åŠ¡"""
        if not self.checkpoint_manager:
            raise ValueError("æ£€æŸ¥ç‚¹åŠŸèƒ½æœªå¯ç”¨ï¼Œæ— æ³•æ¢å¤ä¼šè¯")

        print(f"ğŸ”„ æ¢å¤ä¼šè¯: {session_id}")

        # åŠ è½½æ£€æŸ¥ç‚¹
        try:
            (
                state,
                chapters,
                summaries,
                polished_summaries,
            ) = self.checkpoint_manager.load_checkpoint(session_id)
        except FileNotFoundError:
            print(f"âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {session_id}")
            return

        print(f"ğŸ“Š æ¢å¤çŠ¶æ€: {state.current_stage}")
        print(f"ğŸ“ˆ å·²å®Œæˆ: {len(state.completed_chapter_ids)}/{state.total_chapters}")
        print(f"âŒ å¤±è´¥æ•°é‡: {len(state.failed_chapter_ids)}")

        if state.current_stage == "initial_summary":
            # ç»§ç»­ç”Ÿæˆåˆæ­¥æ€»ç»“
            print("\nç»§ç»­ç”Ÿæˆåˆæ­¥æ€»ç»“...")
            summaries = await self._generate_summaries_with_checkpoint(
                chapters,
                state.compression_level,
                state,
                existing_summaries=summaries,
                debug_pairs=debug_pairs,
                output_path=Path(state.output_path),
            )

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰“ç£¨
            if self.config.polish.enabled and summaries:
                print(f"\nå¼€å§‹äºŒæ¬¡æ‰“ç£¨...")
                state.current_stage = "polish"
                state.processed_chapters = 0
                state.completed_chapter_ids = []
                self.checkpoint_manager.save_checkpoint(state, summaries=summaries)

                polished_summaries = await self._polish_summaries_with_checkpoint(
                    summaries,
                    state.compression_level,
                    state,
                    debug_pairs=debug_pairs,
                    output_path=Path(state.output_path),
                )
                final_summaries = polished_summaries
            else:
                final_summaries = summaries

        elif state.current_stage == "polish":
            # ç»§ç»­äºŒæ¬¡æ‰“ç£¨
            print("\nç»§ç»­äºŒæ¬¡æ‰“ç£¨...")
            polished_summaries = await self._polish_summaries_with_checkpoint(
                summaries,
                state.compression_level,
                state,
                existing_polished=polished_summaries,
                debug_pairs=debug_pairs,
                output_path=Path(state.output_path),
            )
            final_summaries = polished_summaries

        elif state.current_stage == "completed":
            print("âœ… ä¼šè¯å·²å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆè¾“å‡º...")
            final_summaries = polished_summaries if polished_summaries else summaries
        else:
            print(f"âŒ æœªçŸ¥çŠ¶æ€: {state.current_stage}")
            return

        # å®Œæˆå¤„ç†
        await self._finalize_summary(
            final_summaries,
            state,
            chapters,
            Path(state.output_path),
            state.compression_level,
        )

    async def _generate_summaries_with_checkpoint(
        self,
        chapter_chunks: List[DocumentChunk],
        compression_level: int,
        state: Optional[CheckpointState] = None,
        existing_summaries: List[SummaryResult] = None,
        debug_pairs: bool = False,
        output_path: Optional[Path] = None,
    ) -> List[SummaryResult]:
        """å¸¦æ£€æŸ¥ç‚¹çš„æ€»ç»“ç”Ÿæˆ"""

        if compression_level not in self.config.compression_levels:
            raise ValueError(f"ä¸æ”¯æŒçš„å‹ç¼©çº§åˆ«: {compression_level}")

        compression_config = self.config.compression_levels[compression_level]

        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
        summaries = existing_summaries or []
        completed_ids = set(state.completed_chapter_ids if state else [])
        failed_ids = set(state.failed_chapter_ids if state else [])

        # è¿‡æ»¤éœ€è¦å¤„ç†çš„ç« èŠ‚
        pending_chunks = [
            chunk
            for chunk in chapter_chunks
            if chunk.id not in completed_ids and chunk.id not in failed_ids
        ]

        if not pending_chunks:
            print("âœ… æ‰€æœ‰ç« èŠ‚å·²å¤„ç†å®Œæˆ")
            return summaries

        print(
            f"ğŸ“ éœ€è¦å¤„ç† {len(pending_chunks)} ä¸ªç« èŠ‚ (è·³è¿‡ {len(completed_ids)} ä¸ªå·²å®Œæˆ, {len(failed_ids)} ä¸ªå¤±è´¥)"
        )

        # åˆ›å»ºåç«¯
        backend = BackendManager.create_backend(self.config)

        # åˆ›å»ºdebugç›®å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        debug_dir = None
        if debug_pairs and output_path:
            debug_dir = output_path.parent / "debug_pairs" / "initial_summaries"
            debug_dir.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ” Debug: åˆæ­¥æ€»ç»“pairså°†ä¿å­˜åˆ° {debug_dir}")

        # å¤„ç†ç« èŠ‚ - æŒ‰åŸå§‹é¡ºåºå¤„ç†
        async with backend:
            with tqdm(
                total=len(pending_chunks), desc="ç”Ÿæˆæ€»ç»“", initial=len(completed_ids)
            ) as pbar:
                # æŒ‰chunk.idæ’åºç¡®ä¿å¤„ç†é¡ºåºæ­£ç¡®
                pending_chunks_sorted = sorted(pending_chunks, key=lambda x: x.id)
                # é€ä¸ªå¤„ç†ä»¥ä¾¿å®æ—¶ä¿å­˜æ£€æŸ¥ç‚¹
                for chunk in pending_chunks_sorted:
                    try:
                        # æ„å»ºæç¤ºè¯
                        full_prompt = self._build_summary_prompt(
                            chunk, compression_config, compression_level
                        )

                        # è°ƒç”¨åç«¯
                        summary_text = await backend.single_generate(full_prompt)

                        # åˆ›å»ºç»“æœå¯¹è±¡
                        result = SummaryResult(
                            chunk_id=chunk.id,
                            original_content=chunk.content,
                            summary=summary_text,
                            section_title=chunk.chapter_title,
                            section_level=chunk.chapter_level,
                            token_count=len(summary_text.split()),
                            compression_ratio=self._calculate_compression_ratio(
                                chunk.content, summary_text
                            ),
                            metadata={
                                "compression_level": compression_level,
                                "original_token_count": chunk.token_count,
                                "sub_sections": chunk.sub_sections,
                                "backend_type": self.config.backend.type,
                                "polished": False,
                            },
                        )

                        summaries.append(result)

                        # ä¿å­˜debug pairï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if debug_dir:
                            await self._save_debug_pair(
                                debug_dir,
                                chunk.id,
                                chunk.chapter_title,
                                chunk.content,
                                summary_text,
                                "initial_summary",
                                compression_level,
                            )

                        if state:
                            state.completed_chapter_ids.append(chunk.id)
                            state.processed_chapters = len(state.completed_chapter_ids)

                            # æ¯å¤„ç†ä¸€ä¸ªç« èŠ‚å°±ä¿å­˜æ£€æŸ¥ç‚¹
                            self.checkpoint_manager.save_checkpoint(
                                state, summaries=summaries
                            )

                        pbar.update(1)

                    except Exception as e:
                        if state:
                            error_info = {
                                "chunk_id": chunk.id,
                                "chapter_title": chunk.chapter_title,
                                "error": str(e),
                                "timestamp": datetime.now().isoformat(),
                            }
                            state.failed_chapter_ids.append(chunk.id)
                            state.error_log.append(error_info)

                            print(f"\nâŒ ç« èŠ‚å¤„ç†å¤±è´¥: {chunk.chapter_title}")
                            print(f"   é”™è¯¯: {str(e)}")

                            # ä¿å­˜å¤±è´¥çŠ¶æ€
                            self.checkpoint_manager.save_checkpoint(
                                state, summaries=summaries
                            )

                            # å¦‚æœæ˜¯é™æµé”™è¯¯ï¼Œæç¤ºç”¨æˆ·
                            if (
                                "RATE_LIMIT_ERROR" in str(e)
                                or "rate limit" in str(e).lower()
                            ):
                                print(f"â¸ï¸ æ£€æµ‹åˆ°é™æµé”™è¯¯ï¼Œå·²ä¿å­˜æ£€æŸ¥ç‚¹")
                                print(
                                    f"   å¯ä»¥ç¨åè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¢å¤: mineru-summarizer --resume {state.session_id}"
                                )
                                break
                        else:
                            print(f"\nâŒ ç« èŠ‚å¤„ç†å¤±è´¥: {chunk.chapter_title}")
                            print(f"   é”™è¯¯: {str(e)}")

                        pbar.update(1)

        return summaries

    async def _polish_summaries_with_checkpoint(
        self,
        summaries: List[SummaryResult],
        compression_level: int,
        state: Optional[CheckpointState] = None,
        existing_polished: List[SummaryResult] = None,
        debug_pairs: bool = False,
        output_path: Optional[Path] = None,
    ) -> List[SummaryResult]:
        """å¸¦æ£€æŸ¥ç‚¹çš„äºŒæ¬¡æ‰“ç£¨"""

        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
        polished_summaries = existing_polished or []
        completed_ids = set(state.completed_chapter_ids if state else [])
        failed_ids = set(state.failed_chapter_ids if state else [])

        # è¿‡æ»¤éœ€è¦å¤„ç†çš„æ€»ç»“
        pending_summaries = [
            s
            for s in summaries
            if s.chunk_id not in completed_ids and s.chunk_id not in failed_ids
        ]

        if not pending_summaries:
            print("âœ… æ‰€æœ‰æ€»ç»“å·²æ‰“ç£¨å®Œæˆ")
            return polished_summaries

        print(f"ğŸ¨ éœ€è¦æ‰“ç£¨ {len(pending_summaries)} ä¸ªæ€»ç»“")

        # åˆ›å»ºåç«¯ï¼ˆæ‰“ç£¨æ—¶é™ä½å¹¶å‘æ•°ï¼‰
        polish_config = self.config
        polish_config.processing.max_concurrent = max(
            1,
            int(
                self.config.processing.max_concurrent
                * self.config.polish.concurrent_ratio
            ),
        )
        backend = BackendManager.create_backend(polish_config)

        # åˆ›å»ºdebugç›®å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        debug_dir = None
        if debug_pairs and output_path:
            debug_dir = output_path.parent / "debug_pairs" / "polished_summaries"
            debug_dir.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ” Debug: æ‰“ç£¨åæ€»ç»“pairså°†ä¿å­˜åˆ° {debug_dir}")

        # å¤„ç†æ‰“ç£¨
        async with backend:
            with tqdm(
                total=len(pending_summaries), desc="æ‰“ç£¨æ€»ç»“", initial=len(completed_ids)
            ) as pbar:
                for summary in pending_summaries:
                    try:
                        # æ„å»ºæ‰“ç£¨æç¤ºè¯
                        polish_prompt = self._build_polish_prompt(
                            summary, compression_level
                        )

                        # è°ƒç”¨æ‰“ç£¨
                        polished_text = await backend.single_generate(polish_prompt)

                        # åˆ›å»ºæ‰“ç£¨åçš„ç»“æœ
                        polished_result = SummaryResult(
                            chunk_id=summary.chunk_id,
                            original_content=summary.original_content,
                            summary=polished_text,
                            section_title=summary.section_title,
                            section_level=summary.section_level,
                            token_count=len(polished_text.split()),
                            compression_ratio=self._calculate_compression_ratio(
                                summary.original_content, polished_text
                            ),
                            metadata={
                                **summary.metadata,
                                "polished": True,
                                "original_summary_tokens": summary.token_count,
                                "polish_improvement_ratio": len(polished_text.split())
                                / summary.token_count
                                if summary.token_count > 0
                                else 1.0,
                            },
                        )

                        polished_summaries.append(polished_result)

                        # ä¿å­˜debug pairï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if debug_dir:
                            await self._save_debug_pair(
                                debug_dir,
                                summary.chunk_id,
                                summary.section_title,
                                summary.summary,
                                polished_text,
                                "polish",
                                compression_level,
                            )

                        if state:
                            state.completed_chapter_ids.append(summary.chunk_id)
                            state.processed_chapters = len(state.completed_chapter_ids)

                            # ä¿å­˜æ£€æŸ¥ç‚¹
                            self.checkpoint_manager.save_checkpoint(
                                state, polished_summaries=polished_summaries
                            )

                        pbar.update(1)

                    except Exception as e:
                        if state:
                            error_info = {
                                "chunk_id": summary.chunk_id,
                                "chapter_title": summary.section_title,
                                "error": str(e),
                                "timestamp": datetime.now().isoformat(),
                                "stage": "polish",
                            }
                            state.failed_chapter_ids.append(summary.chunk_id)
                            state.error_log.append(error_info)

                            print(f"\nâŒ æ‰“ç£¨å¤±è´¥: {summary.section_title}")
                            print(f"   é”™è¯¯: {str(e)}")

                            # ä¿å­˜å¤±è´¥çŠ¶æ€
                            self.checkpoint_manager.save_checkpoint(
                                state, polished_summaries=polished_summaries
                            )

                            # æ£€æŸ¥é™æµ
                            if (
                                "RATE_LIMIT_ERROR" in str(e)
                                or "rate limit" in str(e).lower()
                            ):
                                print(f"â¸ï¸ æ£€æµ‹åˆ°é™æµé”™è¯¯ï¼Œå·²ä¿å­˜æ£€æŸ¥ç‚¹")
                                print(
                                    f"   å¯ä»¥ç¨åè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¢å¤: mineru-summarizer --resume {state.session_id}"
                                )
                                break
                        else:
                            print(f"\nâŒ æ‰“ç£¨å¤±è´¥: {summary.section_title}")
                            print(f"   é”™è¯¯: {str(e)}")

                        pbar.update(1)

        return polished_summaries

    def _build_summary_prompt(
        self,
        chunk: DocumentChunk,
        compression_config: Dict[str, str],
        compression_level: int,
    ) -> str:
        """æ„å»ºæ€»ç»“æç¤ºè¯"""
        context_info = f"\n\nç« èŠ‚æ ‡é¢˜: {chunk.chapter_title}"
        if chunk.sub_sections:
            context_info += f"\nå­ç« èŠ‚: {', '.join(chunk.sub_sections[:5])}"
            if len(chunk.sub_sections) > 5:
                context_info += f" (è¿˜æœ‰{len(chunk.sub_sections)-5}ä¸ªå­ç« èŠ‚)"
        context_info += f"\nç« èŠ‚è§„æ¨¡: {chunk.token_count:,} tokens"

        full_prompt = f"""{compression_config['prompt_template']}

{context_info}

é‡è¦è¦æ±‚:
1. è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¤§ç« èŠ‚ï¼Œè¯·ä¿æŒå†…å®¹çš„è¿è´¯æ€§å’Œé€»è¾‘å®Œæ•´æ€§
2. é‡ç‚¹çªå‡ºç« èŠ‚çš„æ ¸å¿ƒè´¡çŒ®å’Œä¸»è¦è§‚ç‚¹
3. ä¿æŒå­¦æœ¯å†™ä½œçš„ä¸¥è°¨æ€§å’Œä¸“ä¸šæ€§
4. å¯¹äºåŒ…å«å¤šä¸ªå­ç« èŠ‚çš„å†…å®¹ï¼Œè¯·æŒ‰é€»è¾‘é¡ºåºç»„ç»‡æ€»ç»“
5. å¦‚æœåŸæ–‡ä¸­æœ‰å›¾ç‰‡é“¾æ¥ï¼ˆæ ¼å¼å¦‚![](images/xxx.jpg)ï¼‰ï¼Œè¯·ä¸­ä¿ç•™è¿™äº›å›¾ç‰‡é“¾æ¥ï¼Œå¦‚æœæ–‡ä¸­æœ‰å¯¹å›¾ç‰‡çš„ç›¸å…³æè¿°å¯ä»¥ç®€è¦æåŠã€‚ä¸è¦è¾“å‡ºä»»ä½•åŸæ–‡ä¸­æ²¡æœ‰çš„å›¾ç‰‡é“¾æ¥
7. ä¸“æœ‰æŠ€æœ¯åè¯ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è¯‘ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
   - Graph Neural Networks (GNNs), Graph Foundation Models (GFMs), Transformer
   - Graph Attention Networks (GAT), GraphSAGE, Message Passing, Node Embedding
   - Graph Convolutional Networks (GCN), Self-supervised Learning, Pre-training
   - Fine-tuning, In-context Learning, Few-shot Learning, Zero-shot Learning
   - Knowledge Graph, Heterogeneous Graph, Homogeneous Graph, Graph Isomorphism
   - Graph Pooling, Graph Classification, Node Classification, Link Prediction
   - Graph Generation, Graph Anomaly Detection, Contrastive Learning
   - Multi-modal Learning, Cross-domain Transfer, Domain Adaptation

åŸæ–‡å†…å®¹:
{chunk.content}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½†ä¿æŒä¸Šè¿°è‹±æ–‡æŠ€æœ¯æœ¯è¯­ï¼š"""

        return full_prompt

    def _build_polish_prompt(
        self, summary: SummaryResult, compression_level: int
    ) -> str:
        """æ„å»ºæ‰“ç£¨æç¤ºè¯"""
        polish_prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¼–è¾‘ï¼Œè¯·å¯¹ä»¥ä¸‹ä¸­æ–‡æ€»ç»“è¿›è¡Œæ‰“ç£¨å’Œä¼˜åŒ–ã€‚

ç« èŠ‚æ ‡é¢˜: {summary.section_title}
å‹ç¼©çº§åˆ«: {compression_level}%

æ‰“ç£¨è¦æ±‚:
1. è¯­è¨€æ¶¦è‰²ï¼šæå‡è¡¨è¾¾çš„æµç•…æ€§å’Œå‡†ç¡®æ€§ï¼Œä½¿ç”¨æ›´åœ°é“çš„ä¸­æ–‡å­¦æœ¯è¡¨è¾¾
2. é€»è¾‘ä¼˜åŒ–ï¼šç¡®ä¿è®ºç‚¹æ¸…æ™°ï¼Œé€»è¾‘é“¾æ¡å®Œæ•´ï¼Œé‡ç‚¹çªå‡º
3. æœ¯è¯­ç»Ÿä¸€ï¼šç¡®ä¿ä¸“æœ‰æŠ€æœ¯åè¯ä½¿ç”¨ä¸€è‡´ï¼Œä¿æŒè‹±æ–‡åŸæ–‡
4. ç»“æ„å®Œå–„ï¼šä¼˜åŒ–æ®µè½ç»“æ„ï¼Œæå‡å¯è¯»æ€§
5. ä¿¡æ¯å¯†åº¦ï¼šåœ¨ä¿æŒ{compression_level}%ä¿¡æ¯é‡çš„åŸºç¡€ä¸Šï¼Œæå‡ä¿¡æ¯çš„ä»·å€¼å¯†åº¦
6. å­¦æœ¯è§„èŒƒï¼šç¬¦åˆä¸­æ–‡å­¦æœ¯å†™ä½œè§„èŒƒï¼Œä¿æŒä¸¥è°¨æ€§
7. å¦‚æœåŸæ–‡ä¸­æœ‰å›¾ç‰‡é“¾æ¥ï¼ˆæ ¼å¼å¦‚![](images/xxx.jpg)ï¼‰ï¼Œè¯·ä¸­ä¿ç•™è¿™äº›å›¾ç‰‡é“¾æ¥ï¼Œå¦‚æœæ–‡ä¸­æœ‰å¯¹å›¾ç‰‡çš„ç›¸å…³æè¿°å¯ä»¥ç®€è¦æåŠã€‚ä¸è¦è¾“å‡ºä»»ä½•åŸæ–‡ä¸­æ²¡æœ‰çš„å›¾ç‰‡é“¾æ¥

éœ€è¦ä¿æŒè‹±æ–‡çš„æŠ€æœ¯æœ¯è¯­ï¼ˆè¯·å‹¿ç¿»è¯‘ï¼‰:
Graph Neural Networks (GNNs), Graph Foundation Models (GFMs), Transformer, Graph Attention Networks (GAT), GraphSAGE, Message Passing, Node Embedding, Graph Convolutional Networks (GCN), Self-supervised Learning, Pre-training, Fine-tuning, In-context Learning, Few-shot Learning, Zero-shot Learning, Knowledge Graph, Heterogeneous Graph, Homogeneous Graph, Graph Isomorphism, Graph Pooling, Graph Classification, Node Classification, Link Prediction, Graph Generation, Graph Anomaly Detection, Contrastive Learning, Multi-modal Learning, Cross-domain Transfer, Domain Adaptation

åŸå§‹æ€»ç»“:
{summary.summary}

è¯·è¾“å‡ºæ‰“ç£¨åçš„æ€»ç»“ï¼Œç›´æ¥ç»™å‡ºæœ€ç»ˆç»“æœï¼Œä¸è¦åŒ…å«è¯´æ˜æ€§æ–‡å­—ï¼š"""

        return polish_prompt

    def _calculate_compression_ratio(self, original: str, summary: str) -> float:
        """è®¡ç®—å‹ç¼©æ¯”ä¾‹"""
        original_length = len(original)
        summary_length = len(summary)

        if original_length == 0:
            return 0.0

        return summary_length / original_length

    def _extract_title(self, chunks: List[DocumentChunk]) -> str:
        """ä»æ–‡æ¡£å—ä¸­æå–æ ‡é¢˜"""
        for chunk in chunks:
            if chunk.chapter_level == 1 and not chunk.chapter_title.startswith(
                "Chapter"
            ):
                return chunk.chapter_title
        return "å­¦æœ¯è®ºæ–‡"

    async def _finalize_summary(
        self,
        summaries: List[SummaryResult],
        state: Optional[CheckpointState],
        chapters: List[DocumentChunk],
        output_path: Path,
        compression_level: int,
    ) -> None:
        """å®Œæˆæ€»ç»“å¤„ç†"""
        if not summaries:
            print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆçš„æ€»ç»“")
            return

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        overall_stats = self._calculate_overall_stats(summaries)
        print(f"æ€»ä½“ç»Ÿè®¡: {overall_stats}")

        # æ ¼å¼åŒ–è¾“å‡º
        print(f"\næ­¥éª¤: æ ¼å¼åŒ–è¾“å‡º...")
        formatter = MarkdownFormatterV2(self.config.output)

        # ä»ç¬¬ä¸€ä¸ªå—æå–æ ‡é¢˜
        original_title = self._extract_title(chapters)

        formatted_content = formatter.format_summaries(
            summaries=summaries,
            original_title=f"{original_title} ({'æ‰“ç£¨ç‰ˆ' if self.config.polish.enabled else 'æ ‡å‡†ç‰ˆ'})",
            compression_level=compression_level,
            stats=overall_stats,
        )

        # ä¿å­˜æ–‡ä»¶
        formatter.save_to_file(formatted_content, output_path)

        # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
        if state:
            state.current_stage = "completed"
            self.checkpoint_manager.save_checkpoint(state, summaries=summaries)

        print(f"\nâœ… æ€»ç»“å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"ğŸ“Š åŸå§‹å†…å®¹: {overall_stats.get('total_original_tokens', 0):,} tokens")
        print(f"ğŸ“ æ€»ç»“å†…å®¹: {overall_stats.get('total_summary_tokens', 0):,} tokens")
        print(f"ğŸ¯ å‹ç¼©æ¯”: {overall_stats.get('overall_compression_ratio', 0):.2%}")
        if state:
            print(f"ğŸ’¾ ä¼šè¯ID: {state.session_id}")

        if state and state.failed_chapter_ids:
            print(f"âš ï¸ å¤±è´¥ç« èŠ‚æ•°: {len(state.failed_chapter_ids)}")
            print("   å¯ä»¥æŸ¥çœ‹æ£€æŸ¥ç‚¹æ–‡ä»¶äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯")

    def _calculate_overall_stats(
        self, summaries: List[SummaryResult]
    ) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""
        if not summaries:
            return {}

        total_original_tokens = sum(
            s.metadata.get("original_token_count", 0) for s in summaries
        )
        total_summary_tokens = sum(s.token_count for s in summaries)

        overall_compression_ratio = (
            total_summary_tokens / total_original_tokens
            if total_original_tokens > 0
            else 0
        )

        return {
            "total_chunks": len(summaries),
            "total_original_tokens": total_original_tokens,
            "total_summary_tokens": total_summary_tokens,
            "overall_compression_ratio": overall_compression_ratio,
            "average_compression_ratio": sum(s.compression_ratio for s in summaries)
            / len(summaries),
            "sections_processed": len(summaries),
        }

    def list_checkpoints(self) -> None:
        """åˆ—å‡ºæ£€æŸ¥ç‚¹"""
        if not self.checkpoint_manager:
            print("âŒ æ£€æŸ¥ç‚¹åŠŸèƒ½æœªå¯ç”¨")
            return

        checkpoints = self.checkpoint_manager.list_checkpoints()
        print_checkpoints_table(checkpoints)

    def clean_checkpoints(self, keep_days: int = 7) -> None:
        """æ¸…ç†æ£€æŸ¥ç‚¹"""
        if not self.checkpoint_manager:
            print("âŒ æ£€æŸ¥ç‚¹åŠŸèƒ½æœªå¯ç”¨")
            return

        self.checkpoint_manager.clean_old_checkpoints(keep_days)

    async def _save_debug_pair(
        self,
        debug_dir: Path,
        chunk_id: str,
        title: str,
        original_text: str,
        summary_text: str,
        stage: str,
        compression_level: int,
    ) -> None:
        """ä¿å­˜debug pairåˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
            safe_title = "".join(
                c for c in title if c.isalnum() or c in (" ", "-", "_")
            ).strip()
            safe_title = safe_title.replace(" ", "_")[:50]  # é™åˆ¶é•¿åº¦

            filename = f"{chunk_id}_{safe_title}.md"
            file_path = debug_dir / filename

            # åˆ›å»ºmarkdownæ ¼å¼çš„å†…å®¹
            content = f"""# Debug Pair: {title}

**Chunk ID:** {chunk_id}  
**Stage:** {stage}  
**Compression Level:** {compression_level}%  
**Timestamp:** {datetime.now().isoformat()}

## åŸå§‹å†…å®¹

```markdown
{original_text}
```

## æ€»ç»“å†…å®¹

```markdown
{summary_text}
```

## ç»Ÿè®¡ä¿¡æ¯

- åŸå§‹å­—ç¬¦æ•°: {len(original_text):,}
- æ€»ç»“å­—ç¬¦æ•°: {len(summary_text):,}
- å‹ç¼©æ¯”: {len(summary_text) / len(original_text) * 100:.1f}%
- åŸå§‹tokenæ•°(ä¼°ç®—): {len(original_text.split()):,}
- æ€»ç»“tokenæ•°(ä¼°ç®—): {len(summary_text.split()):,}

---
*Generated by MinerU Summarizer Debug Mode*
"""

            # å†™å…¥æ–‡ä»¶
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜debug pairå¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹

    async def _create_debug_index(
        self, output_path: Path, summaries: List[SummaryResult]
    ) -> None:
        """åˆ›å»ºdebugç´¢å¼•æ–‡ä»¶"""
        try:
            debug_base_dir = output_path.parent / "debug_pairs"
            index_path = debug_base_dir / "INDEX.md"

            # ç”Ÿæˆç´¢å¼•å†…å®¹
            content = f"""# Debug Pairs ç´¢å¼•

**ç”Ÿæˆæ—¶é—´:** {datetime.now().isoformat()}  
**è¾“å‡ºæ–‡ä»¶:** {output_path.name}  
**æ€»ç»“æ•°é‡:** {len(summaries)}

## æ–‡ä»¶ç»“æ„

```
debug_pairs/
â”œâ”€â”€ INDEX.md                    # æœ¬ç´¢å¼•æ–‡ä»¶
â”œâ”€â”€ initial_summaries/          # åˆæ­¥æ€»ç»“çš„(åŸæ–‡,æ€»ç»“)å¯¹
â”‚   â”œâ”€â”€ chapter_001_*.md
â”‚   â””â”€â”€ ...
â””â”€â”€ polished_summaries/         # æ‰“ç£¨åçš„(åˆæ­¥æ€»ç»“,æ‰“ç£¨æ€»ç»“)å¯¹
    â”œâ”€â”€ chapter_001_*.md
    â””â”€â”€ ...
```

## åˆ†ç‰‡æ€»ç»“åˆ—è¡¨

| åºå· | Chunk ID | æ ‡é¢˜ | çŠ¶æ€ | åˆæ­¥æ€»ç»“æ–‡ä»¶ | æ‰“ç£¨æ€»ç»“æ–‡ä»¶ |
|------|----------|------|------|-------------|-------------|
"""

            # æ·»åŠ æ¯ä¸ªåˆ†ç‰‡çš„ä¿¡æ¯
            initial_dir = debug_base_dir / "initial_summaries"
            polished_dir = debug_base_dir / "polished_summaries"

            for i, summary in enumerate(summaries, 1):
                # åˆ›å»ºå®‰å…¨çš„æ ‡é¢˜
                safe_title = "".join(
                    c
                    for c in summary.section_title
                    if c.isalnum() or c in (" ", "-", "_")
                ).strip()
                safe_title = safe_title.replace(" ", "_")[:50]

                initial_file = f"{summary.chunk_id}_{safe_title}.md"
                polished_file = (
                    f"{summary.chunk_id}_{safe_title}.md"
                    if summary.metadata.get("polished", False)
                    else "N/A"
                )

                status = "å·²æ‰“ç£¨" if summary.metadata.get("polished", False) else "ä»…åˆæ­¥æ€»ç»“"

                initial_link = (
                    f"[{initial_file}](initial_summaries/{initial_file})"
                    if (initial_dir / initial_file).exists()
                    else "N/A"
                )
                polished_link = (
                    f"[{polished_file}](polished_summaries/{polished_file})"
                    if polished_file != "N/A"
                    and (polished_dir / polished_file).exists()
                    else "N/A"
                )

                content += f"| {i} | {summary.chunk_id} | {summary.section_title} | {status} | {initial_link} | {polished_link} |\n"

            content += f"""

## ä½¿ç”¨è¯´æ˜

1. **åˆæ­¥æ€»ç»“æ–‡ä»¶å¤¹** (`initial_summaries/`): åŒ…å«åŸå§‹ç« èŠ‚å†…å®¹å’Œå¯¹åº”çš„åˆæ­¥æ€»ç»“
2. **æ‰“ç£¨æ€»ç»“æ–‡ä»¶å¤¹** (`polished_summaries/`): åŒ…å«åˆæ­¥æ€»ç»“å’Œç»è¿‡æ‰“ç£¨åçš„æœ€ç»ˆæ€»ç»“
3. æ¯ä¸ªæ–‡ä»¶éƒ½åŒ…å«è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬å­—ç¬¦æ•°ã€tokenæ•°ã€å‹ç¼©æ¯”ç­‰
4. æ–‡ä»¶å‘½åæ ¼å¼: `{{chunk_id}}_{{safe_title}}.md`

## ç»Ÿè®¡æ±‡æ€»

- æ€»åˆ†ç‰‡æ•°: {len(summaries)}
- å·²æ‰“ç£¨åˆ†ç‰‡æ•°: {sum(1 for s in summaries if s.metadata.get('polished', False))}
- å¹³å‡å‹ç¼©æ¯”: {sum(s.compression_ratio for s in summaries) / len(summaries) * 100:.1f}%

---
*Generated by MinerU Summarizer Debug Mode*
"""

            # å†™å…¥ç´¢å¼•æ–‡ä»¶
            with open(index_path, "w", encoding="utf-8") as f:
                f.write(content)

            print(f"ğŸ” Debugç´¢å¼•æ–‡ä»¶å·²ç”Ÿæˆ: {index_path}")

        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆdebugç´¢å¼•å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
