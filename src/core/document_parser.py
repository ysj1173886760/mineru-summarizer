from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from langchain_text_splitters import MarkdownHeaderTextSplitter
import logging

logger = logging.getLogger(__name__)


def count_tokens(text: str) -> int:
    """ç®€å•çš„tokenè®¡æ•°"""
    return len(text.split())


@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—"""
    id: str
    content: str
    metadata: Dict[str, Any]
    token_count: int
    chapter_title: str
    chapter_level: int
    sub_sections: List[str]


class DocumentParser:
    """ç»Ÿä¸€çš„æ–‡æ¡£è§£æå™¨ - åŸºäºV3çš„markdown headeråˆ†å‰²"""
    
    def __init__(self, max_tokens_per_chapter: int = 8000):
        self.max_tokens_per_chapter = max_tokens_per_chapter
        
        # åªæŒ‰ä¸€çº§æ ‡é¢˜åˆ†å‰²ï¼ˆV3ç‰¹æ€§ï¼‰
        self.headers_to_split_on = [
            ("#", "Header 1"),
        ]
        
        # åˆ›å»ºmarkdownæ ‡é¢˜åˆ†å‰²å™¨
        self.markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=self.headers_to_split_on,
            strip_headers=False
        )
    
    def parse_markdown_file(self, file_path: Path) -> List[DocumentChunk]:
        """è§£æMarkdownæ–‡ä»¶ï¼Œä¿æŒç« èŠ‚çš„åŸå§‹é¡ºåº"""
        
        # è¯»å–markdownå†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        
        # æ¸…ç†å†…å®¹
        markdown_content = self._clean_markdown(markdown_content)
        
        # æŒ‰ä¸€çº§æ ‡é¢˜åˆ†å‰²
        header_chunks = self.markdown_splitter.split_text(markdown_content)
        
        final_chunks = []
        skipped_chapters = []
        chunk_index = 0  # ç”¨äºä¿æŒé¡ºåºçš„ç´¢å¼•
        
        for i, chunk in enumerate(header_chunks):
            # æå–ç« èŠ‚ä¿¡æ¯
            chapter_title = chunk.metadata.get('Header 1', f'Chapter {i+1}')
            
            # è¿‡æ»¤ä¸éœ€è¦æ€»ç»“çš„ç« èŠ‚
            if self._should_skip_chapter(chapter_title):
                skipped_chapters.append(chapter_title)
                continue
            
            token_count = count_tokens(chunk.page_content)
            sub_sections = self._extract_sub_sections(chunk.page_content)
            
            if token_count <= self.max_tokens_per_chapter:
                # ç« èŠ‚å¤§å°åˆé€‚ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¿æŒåŸå§‹é¡ºåº
                chapter_chunk = DocumentChunk(
                    id=f"chapter_{chunk_index:03d}",  # ä½¿ç”¨3ä½æ•°ç¼–å·ç¡®ä¿æ’åºæ­£ç¡®
                    content=chunk.page_content,
                    metadata=chunk.metadata,
                    token_count=token_count,
                    chapter_title=chapter_title,
                    chapter_level=1,
                    sub_sections=sub_sections
                )
                final_chunks.append(chapter_chunk)
                chunk_index += 1
            else:
                # ç« èŠ‚å¤ªå¤§ï¼ŒæŒ‰äºŒçº§æ ‡é¢˜åˆ†å‰²ï¼Œä¿æŒåŸå§‹é¡ºåº
                sub_chunks = self._split_large_chapter(chunk, chunk_index)
                final_chunks.extend(sub_chunks)
                chunk_index += len(sub_chunks)
        
        if skipped_chapters:
            print(f"ğŸš« è·³è¿‡çš„ç« èŠ‚: {', '.join(skipped_chapters)}")
        
        return final_chunks
    
    def _should_skip_chapter(self, chapter_title: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æŸä¸ªç« èŠ‚"""
        skip_keywords = [
            'references',      # å‚è€ƒæ–‡çŒ®
            'bibliography',    # å‚è€ƒä¹¦ç›®
            'contents',        # ç›®å½•
            'table of contents', # ç›®å½•
            'appendix',        # é™„å½•
            'acknowledgments', # è‡´è°¢
            'acknowledgements', # è‡´è°¢
        ]
        
        title_lower = chapter_title.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è·³è¿‡å…³é”®è¯
        for keyword in skip_keywords:
            if keyword in title_lower:
                return True
        
        # æ£€æŸ¥æ˜¯å¦åªæ˜¯ç« èŠ‚æ ‡é¢˜é¡µï¼ˆé€šå¸¸å¾ˆçŸ­ä¸”åªåŒ…å«æ ‡é¢˜ï¼‰
        if len(chapter_title.strip()) < 5:
            return True
            
        return False
    
    def _split_large_chapter(self, chunk, chapter_index: int) -> List[DocumentChunk]:
        """åˆ†å‰²è¿‡å¤§çš„ç« èŠ‚ï¼Œä¿æŒåŸå§‹é¡ºåº"""
        # åˆ›å»ºåŒ…å«äºŒçº§æ ‡é¢˜çš„åˆ†å‰²å™¨
        sub_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
            ],
            strip_headers=False
        )
        
        sub_chunks = sub_splitter.split_text(chunk.page_content)
        result = []
        
        chapter_title = chunk.metadata.get('Header 1', f'Chapter {chapter_index+1}')
        
        for j, sub_chunk in enumerate(sub_chunks):
            token_count = count_tokens(sub_chunk.page_content)
            sub_title = sub_chunk.metadata.get('Header 2', f'Section {j+1}')
            sub_sections = self._extract_sub_sections(sub_chunk.page_content)
            
            chapter_chunk = DocumentChunk(
                id=f"chapter_{chapter_index:03d}_{j:03d}",  # ä½¿ç”¨3ä½æ•°ç¼–å·ç¡®ä¿æ’åºæ­£ç¡®
                content=sub_chunk.page_content,
                metadata=sub_chunk.metadata,
                token_count=token_count,
                chapter_title=f"{chapter_title} - {sub_title}",
                chapter_level=2,
                sub_sections=sub_sections
            )
            result.append(chapter_chunk)
        
        return result
    
    def _extract_sub_sections(self, content: str) -> List[str]:
        """æå–å†…å®¹ä¸­çš„å­ç« èŠ‚æ ‡é¢˜"""
        lines = content.split('\n')
        sub_sections = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('##') and not line.startswith('###'):
                # äºŒçº§æ ‡é¢˜
                title = line.replace('##', '').strip()
                sub_sections.append(title)
            elif line.startswith('###'):
                # ä¸‰çº§æ ‡é¢˜
                title = line.replace('###', '').strip()
                sub_sections.append(f"  {title}")  # ç¼©è¿›è¡¨ç¤ºå±‚çº§
        
        return sub_sections
    
    def _clean_markdown(self, content: str) -> str:
        """æ¸…ç†markdownå†…å®¹"""
        lines = content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # è·³è¿‡ç©ºè¡Œå¤ªå¤šçš„æƒ…å†µ
            if line.strip():
                cleaned_lines.append(line)
            elif not cleaned_lines or cleaned_lines[-1].strip():
                # ä¿ç•™å¿…è¦çš„ç©ºè¡Œ
                cleaned_lines.append('')
        
        return '\n'.join(cleaned_lines)
    
    def get_statistics(self, chunks: List[DocumentChunk]) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_chapters': len(chunks),
            'total_tokens': sum(chunk.token_count for chunk in chunks),
            'chapters': [],
            'token_distribution': {}
        }
        
        for chunk in chunks:
            chapter_info = {
                'title': chunk.chapter_title,
                'level': chunk.chapter_level,
                'tokens': chunk.token_count,
                'sub_sections_count': len(chunk.sub_sections),
                'sub_sections': chunk.sub_sections[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
            }
            stats['chapters'].append(chapter_info)
        
        # ç»Ÿè®¡tokenåˆ†å¸ƒ
        token_ranges = [(0, 1000), (1000, 3000), (3000, 5000), (5000, 10000), (10000, float('inf'))]
        for start, end in token_ranges:
            range_key = f"{start}-{end if end != float('inf') else 'inf'}"
            count = len([c for c in chunks if start <= c.token_count < end])
            stats['token_distribution'][range_key] = count
        
        return stats
    
    def print_statistics(self, chunks: List[DocumentChunk]) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_statistics(chunks)
        
        print("ğŸ“š æ–‡æ¡£è§£æç»Ÿè®¡:")
        print(f"   æ€»ç« èŠ‚æ•°: {stats['total_chapters']}")
        print(f"   æ€»tokenæ•°: {stats['total_tokens']:,}")
        
        print("\nğŸ“– ç« èŠ‚è¯¦æƒ…:")
        for i, chapter in enumerate(stats['chapters'][:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            level_prefix = "  " * (chapter['level'] - 1)
            print(f"   {i+1}. {level_prefix}{chapter['title']}")
            print(f"      Tokens: {chapter['tokens']:,}, å­ç« èŠ‚: {chapter['sub_sections_count']}")
            if chapter['sub_sections']:
                for sub in chapter['sub_sections'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå­ç« èŠ‚
                    print(f"         â€¢ {sub}")
        
        if len(stats['chapters']) > 10:
            print(f"   ... è¿˜æœ‰ {len(stats['chapters']) - 10} ä¸ªç« èŠ‚")
        
        print("\nğŸ“Š Tokenåˆ†å¸ƒ:")
        for range_key, count in stats['token_distribution'].items():
            print(f"   {range_key} tokens: {count} ä¸ªç« èŠ‚")