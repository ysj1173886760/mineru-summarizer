import asyncio
from pathlib import Path
from typing import Optional
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.parser.chapter_parser import ChapterBasedParser, ChapterChunk
from src.summarizer.llm_client import LLMManager
from src.summarizer.summary_generator import SummaryResult, SummaryPostProcessor
from src.output.markdown_formatter_v2 import MarkdownFormatterV2
from src.utils.config import load_config, Config
from tqdm import tqdm


class MinerUSummarizerV4:
    """MinerUå†…å®¹æ€»ç»“å™¨ V4ç‰ˆæœ¬ - å¤§ç« èŠ‚åˆ†ç‰‡ + äºŒæ¬¡æ‰“ç£¨"""
    
    def __init__(self, config: Config):
        self.config = config
        
    async def summarize(
        self, 
        input_dir: Path, 
        output_path: Path,
        compression_level: int = 50,
        max_tokens_per_chapter: int = 8000,
        enable_polish: bool = True
    ) -> None:
        """æ‰§è¡Œæ€»ç»“ä»»åŠ¡"""
        print(f"ğŸš€ å¼€å§‹å¤„ç†MinerUæ•°æ® (V4ç‰ˆæœ¬ - å¤§ç« èŠ‚åˆ†ç‰‡ + äºŒæ¬¡æ‰“ç£¨): {input_dir}")
        print(f"ğŸ“Š å‹ç¼©çº§åˆ«: {compression_level}%")
        print(f"ğŸ“„ è¾“å‡ºè·¯å¾„: {output_path}")
        print(f"ğŸ“š æœ€å¤§ç« èŠ‚tokenæ•°: {max_tokens_per_chapter:,}")
        print(f"âœ¨ äºŒæ¬¡æ‰“ç£¨: {'å¯ç”¨' if enable_polish else 'ç¦ç”¨'}")
        
        # 1. è§£æMarkdownæ–‡æ¡£
        print("\næ­¥éª¤1: æŒ‰å¤§ç« èŠ‚è§£æMarkdownæ–‡æ¡£...")
        full_md_path = input_dir / "full.md"
        if not full_md_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°full.mdæ–‡ä»¶: {full_md_path}")
        
        parser = ChapterBasedParser(max_tokens_per_chapter=max_tokens_per_chapter)
        chapter_chunks = parser.parse_markdown_file(full_md_path)
        
        print(f"è§£æå®Œæˆ: {len(chapter_chunks)} ä¸ªå¤§ç« èŠ‚")
        parser.print_chapter_info(chapter_chunks)
        
        # 2. ç”Ÿæˆåˆæ­¥æ€»ç»“
        print(f"\næ­¥éª¤2: ç”Ÿæˆåˆæ­¥æ€»ç»“...")
        summaries = await self._generate_summaries(chapter_chunks, compression_level)
        print(f"åˆæ­¥æ€»ç»“ç”Ÿæˆå®Œæˆ: {len(summaries)} ä¸ªæ€»ç»“å—")
        
        # 3. äºŒæ¬¡æ‰“ç£¨ï¼ˆå¯é€‰ï¼‰
        if enable_polish:
            print(f"\næ­¥éª¤3: äºŒæ¬¡æ‰“ç£¨æ€»ç»“...")
            polished_summaries = await self._polish_summaries(summaries, compression_level)
            print(f"æ‰“ç£¨å®Œæˆ: {len(polished_summaries)} ä¸ªç²¾åŒ–æ€»ç»“")
            final_summaries = polished_summaries
        else:
            final_summaries = summaries
        
        # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        overall_stats = SummaryPostProcessor.calculate_overall_stats(final_summaries)
        print(f"æ€»ä½“ç»Ÿè®¡: {overall_stats}")
        
        # 5. æ ¼å¼åŒ–è¾“å‡º
        print(f"\næ­¥éª¤4: æ ¼å¼åŒ–è¾“å‡º...")
        formatter = MarkdownFormatterV2(self.config.output)
        
        # ä»ç¬¬ä¸€ä¸ªå—æå–æ ‡é¢˜
        original_title = self._extract_title(chapter_chunks)
        
        formatted_content = formatter.format_summaries(
            summaries=final_summaries,
            original_title=f"{original_title} (å¤§ç« èŠ‚åˆ†ç‰‡{'+ äºŒæ¬¡æ‰“ç£¨' if enable_polish else ''}ç‰ˆ)",
            compression_level=compression_level,
            stats=overall_stats
        )
        
        # 6. ä¿å­˜æ–‡ä»¶
        formatter.save_to_file(formatted_content, output_path)
        
        print(f"\nâœ… æ€»ç»“å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"ğŸ“Š åŸå§‹å†…å®¹: {overall_stats.get('total_original_tokens', 0):,} tokens")
        print(f"ğŸ“ æ€»ç»“å†…å®¹: {overall_stats.get('total_summary_tokens', 0):,} tokens") 
        print(f"ğŸ¯ å‹ç¼©æ¯”: {overall_stats.get('overall_compression_ratio', 0):.2%}")
    
    async def _generate_summaries(
        self, 
        chapter_chunks: list[ChapterChunk], 
        compression_level: int
    ) -> list[SummaryResult]:
        """ç”Ÿæˆåˆæ­¥æ€»ç»“"""
        
        if compression_level not in self.config.compression_levels:
            raise ValueError(f"ä¸æ”¯æŒçš„å‹ç¼©çº§åˆ«: {compression_level}")
        
        compression_config = self.config.compression_levels[compression_level]
        
        print(f"å¼€å§‹ç”Ÿæˆåˆæ­¥æ€»ç»“ (å‹ç¼©çº§åˆ«: {compression_level}%)")
        print(f"æ€»å…±éœ€è¦å¤„ç† {len(chapter_chunks)} ä¸ªå¤§ç« èŠ‚")
        
        # ç”Ÿæˆæç¤ºè¯
        prompts = []
        for chunk in chapter_chunks:
            # æ„å»ºç« èŠ‚ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = f"\n\nç« èŠ‚æ ‡é¢˜: {chunk.chapter_title}"
            if chunk.sub_sections:
                context_info += f"\nå­ç« èŠ‚: {', '.join(chunk.sub_sections[:5])}"
                if len(chunk.sub_sections) > 5:
                    context_info += f" (è¿˜æœ‰{len(chunk.sub_sections)-5}ä¸ªå­ç« èŠ‚)"
            context_info += f"\nç« èŠ‚è§„æ¨¡: {chunk.token_count:,} tokens"
            
            # æ„å»ºå®Œæ•´æç¤ºè¯
            full_prompt = f"""{compression_config.prompt_template}

{context_info}

é‡è¦è¦æ±‚:
1. è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¤§ç« èŠ‚ï¼Œè¯·ä¿æŒå†…å®¹çš„è¿è´¯æ€§å’Œé€»è¾‘å®Œæ•´æ€§
2. é‡ç‚¹çªå‡ºç« èŠ‚çš„æ ¸å¿ƒè´¡çŒ®å’Œä¸»è¦è§‚ç‚¹
3. ä¿æŒå­¦æœ¯å†™ä½œçš„ä¸¥è°¨æ€§å’Œä¸“ä¸šæ€§
4. å¯¹äºåŒ…å«å¤šä¸ªå­ç« èŠ‚çš„å†…å®¹ï¼Œè¯·æŒ‰é€»è¾‘é¡ºåºç»„ç»‡æ€»ç»“
5. ä¸“æœ‰æŠ€æœ¯åè¯ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è¯‘ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
   - Graph Neural Networks (GNNs)
   - Graph Foundation Models (GFMs) 
   - Transformer
   - Graph Attention Networks (GAT)
   - GraphSAGE
   - Message Passing
   - Node Embedding
   - Graph Convolutional Networks (GCN)
   - Self-supervised Learning
   - Pre-training
   - Fine-tuning
   - In-context Learning
   - Few-shot Learning
   - Zero-shot Learning
   - Knowledge Graph
   - Heterogeneous Graph
   - Homogeneous Graph
   - Graph Isomorphism
   - Graph Pooling
   - Graph Classification
   - Node Classification
   - Link Prediction
   - Graph Generation
   - Graph Anomaly Detection
   - Contrastive Learning
   - Multi-modal Learning
   - Cross-domain Transfer
   - Domain Adaptation

åŸæ–‡å†…å®¹:
{chunk.content}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½†ä¿æŒä¸Šè¿°è‹±æ–‡æŠ€æœ¯æœ¯è¯­ï¼š"""
            
            prompts.append(full_prompt)
        
        # æ‰¹é‡è°ƒç”¨LLM
        summaries = []
        async with LLMManager(
            self.config.llm, 
            max_concurrent=self.config.processing.max_concurrent
        ) as llm_manager:
            
            with tqdm(total=len(prompts), desc="ç”Ÿæˆåˆæ­¥æ€»ç»“") as pbar:
                batch_size = self.config.processing.max_concurrent
                
                for i in range(0, len(prompts), batch_size):
                    batch_prompts = prompts[i:i+batch_size]
                    batch_chunks = chapter_chunks[i:i+batch_size]
                    
                    batch_results = await llm_manager.batch_generate(batch_prompts)
                    
                    # åˆ›å»ºç»“æœå¯¹è±¡
                    for chunk, summary in zip(batch_chunks, batch_results):
                        result = SummaryResult(
                            chunk_id=chunk.id,
                            original_content=chunk.content,
                            summary=summary,
                            section_title=chunk.chapter_title,
                            section_level=chunk.chapter_level,
                            token_count=len(summary.split()),
                            compression_ratio=self._calculate_compression_ratio(
                                chunk.content, summary
                            ),
                            metadata={
                                'compression_level': compression_level,
                                'strategy': compression_config.strategy,
                                'original_token_count': chunk.token_count,
                                'sub_sections': chunk.sub_sections,
                                'chapter_parser': 'V4',
                                'polished': False
                            }
                        )
                        summaries.append(result)
                    
                    pbar.update(len(batch_prompts))
        
        return summaries
    
    async def _polish_summaries(
        self,
        summaries: list[SummaryResult],
        compression_level: int
    ) -> list[SummaryResult]:
        """äºŒæ¬¡æ‰“ç£¨æ€»ç»“"""
        
        print(f"å¼€å§‹äºŒæ¬¡æ‰“ç£¨ {len(summaries)} ä¸ªæ€»ç»“")
        
        # ç”Ÿæˆæ‰“ç£¨æç¤ºè¯
        polish_prompts = []
        for summary in summaries:
            polish_prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¼–è¾‘ï¼Œè¯·å¯¹ä»¥ä¸‹ä¸­æ–‡æ€»ç»“è¿›è¡Œæ‰“ç£¨å’Œä¼˜åŒ–ã€‚

ç« èŠ‚æ ‡é¢˜: {summary.section_title}
å‹ç¼©çº§åˆ«: {compression_level}%

æ‰“ç£¨è¦æ±‚:
1. è¯­è¨€æ¶¦è‰²ï¼šæå‡è¡¨è¾¾çš„æµç•…æ€§å’Œå‡†ç¡®æ€§ï¼Œä½¿ç”¨æ›´åœ°é“çš„ä¸­æ–‡å­¦æœ¯è¡¨è¾¾
2. é€»è¾‘ä¼˜åŒ–ï¼šç¡®ä¿è®ºç‚¹æ¸…æ™°ï¼Œé€»è¾‘é“¾æ¡å®Œæ•´ï¼Œé‡ç‚¹çªå‡º
3. æœ¯è¯­ç»Ÿä¸€ï¼šç¡®ä¿ä¸“æœ‰æŠ€æœ¯åè¯ä½¿ç”¨ä¸€è‡´ï¼Œä¿æŒè‹±æ–‡åŸæ–‡
4. ç»“æ„å®Œå–„ï¼šä¼˜åŒ–æ®µè½ç»“æ„ï¼Œæå‡å¯è¯»æ€§
5. ä¿¡æ¯å¯†åº¦ï¼šåœ¨ä¿æŒ{compression_level}%ä¿¡æ¯é‡çš„åŸºç¡€ä¸Šï¼Œæå‡ä¿¡æ¯çš„ä»·å€¼å¯†åº¦
6. å­¦æœ¯è§„èŒƒï¼šç¬¦åˆä¸­æ–‡å­¦æœ¯å†™ä½œè§„èŒƒï¼Œä¿æŒä¸¥è°¨æ€§

éœ€è¦ä¿æŒè‹±æ–‡çš„æŠ€æœ¯æœ¯è¯­ï¼ˆè¯·å‹¿ç¿»è¯‘ï¼‰:
Graph Neural Networks (GNNs), Graph Foundation Models (GFMs), Transformer, Graph Attention Networks (GAT), GraphSAGE, Message Passing, Node Embedding, Graph Convolutional Networks (GCN), Self-supervised Learning, Pre-training, Fine-tuning, In-context Learning, Few-shot Learning, Zero-shot Learning, Knowledge Graph, Heterogeneous Graph, Homogeneous Graph, Graph Isomorphism, Graph Pooling, Graph Classification, Node Classification, Link Prediction, Graph Generation, Graph Anomaly Detection, Contrastive Learning, Multi-modal Learning, Cross-domain Transfer, Domain Adaptation

åŸå§‹æ€»ç»“:
{summary.summary}

è¯·è¾“å‡ºæ‰“ç£¨åçš„æ€»ç»“ï¼Œç›´æ¥ç»™å‡ºæœ€ç»ˆç»“æœï¼Œä¸è¦åŒ…å«è¯´æ˜æ€§æ–‡å­—ï¼š"""
            
            polish_prompts.append(polish_prompt)
        
        # æ‰¹é‡æ‰“ç£¨
        polished_summaries = []
        async with LLMManager(
            self.config.llm, 
            max_concurrent=max(1, self.config.processing.max_concurrent // 2)  # é™ä½å¹¶å‘ä»¥æé«˜è´¨é‡
        ) as llm_manager:
            
            with tqdm(total=len(polish_prompts), desc="æ‰“ç£¨æ€»ç»“") as pbar:
                batch_size = max(1, self.config.processing.max_concurrent // 2)
                
                for i in range(0, len(polish_prompts), batch_size):
                    batch_prompts = polish_prompts[i:i+batch_size]
                    batch_summaries = summaries[i:i+batch_size]
                    
                    batch_results = await llm_manager.batch_generate(batch_prompts)
                    
                    # åˆ›å»ºæ‰“ç£¨åçš„ç»“æœå¯¹è±¡
                    for original_summary, polished_text in zip(batch_summaries, batch_results):
                        polished_result = SummaryResult(
                            chunk_id=original_summary.chunk_id,
                            original_content=original_summary.original_content,
                            summary=polished_text,
                            section_title=original_summary.section_title,
                            section_level=original_summary.section_level,
                            token_count=len(polished_text.split()),
                            compression_ratio=self._calculate_compression_ratio(
                                original_summary.original_content, polished_text
                            ),
                            metadata={
                                **original_summary.metadata,
                                'polished': True,
                                'original_summary_tokens': original_summary.token_count,
                                'polish_improvement_ratio': len(polished_text.split()) / original_summary.token_count if original_summary.token_count > 0 else 1.0
                            }
                        )
                        polished_summaries.append(polished_result)
                    
                    pbar.update(len(batch_prompts))
        
        return polished_summaries
    
    def _calculate_compression_ratio(self, original: str, summary: str) -> float:
        """è®¡ç®—å‹ç¼©æ¯”ä¾‹"""
        original_length = len(original)
        summary_length = len(summary)
        
        if original_length == 0:
            return 0.0
        
        return summary_length / original_length
    
    def _extract_title(self, chunks: list[ChapterChunk]) -> str:
        """ä»æ–‡æ¡£å—ä¸­æå–æ ‡é¢˜"""
        for chunk in chunks:
            if chunk.chapter_level == 1 and not chunk.chapter_title.startswith('Chapter'):
                return chunk.chapter_title
        return "å­¦æœ¯è®ºæ–‡"


async def main():
    """ä¸»å‡½æ•°"""
    # ç®€å•çš„å‘½ä»¤è¡Œå‚æ•°è§£æ
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python -m src.main_v4 <è¾“å…¥ç›®å½•> <è¾“å‡ºæ–‡ä»¶> [å‹ç¼©çº§åˆ«] [æœ€å¤§ç« èŠ‚tokenæ•°] [æ˜¯å¦æ‰“ç£¨] [é…ç½®æ–‡ä»¶]")
        print("ç¤ºä¾‹: python -m src.main_v4 ./GFM_SURVEY ./summary_v4.md 30 8000 true")
        sys.exit(1)
    
    input_dir = Path(sys.argv[1])
    output_path = Path(sys.argv[2])
    compression_level = int(sys.argv[3]) if len(sys.argv) > 3 else 50
    max_tokens_per_chapter = int(sys.argv[4]) if len(sys.argv) > 4 else 8000
    enable_polish = sys.argv[5].lower() == 'true' if len(sys.argv) > 5 else True
    config_path = sys.argv[6] if len(sys.argv) > 6 else None
    
    # éªŒè¯è¾“å…¥ç›®å½•
    if not input_dir.exists():
        print(f"é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        sys.exit(1)
    
    full_md_path = input_dir / "full.md"
    if not full_md_path.exists():
        print(f"é”™è¯¯: æœªæ‰¾åˆ°full.mdæ–‡ä»¶: {full_md_path}")
        sys.exit(1)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config(config_path)
        print(f"âœ… é…ç½®åŠ è½½å®Œæˆ: LLMæä¾›å•†={config.llm.provider}, æ¨¡å‹={config.llm.model}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: é…ç½®åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    # éªŒè¯APIå¯†é’¥
    if not config.llm.api_key:
        print("âŒ é”™è¯¯: æœªé…ç½®APIå¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶")
        print("OpenAI: è®¾ç½® OPENAI_API_KEY")
        print("Anthropic: è®¾ç½® ANTHROPIC_API_KEY")
        sys.exit(1)
    
    # åˆ›å»ºæ€»ç»“å™¨å¹¶æ‰§è¡Œ
    try:
        summarizer = MinerUSummarizerV4(config)
        await summarizer.summarize(
            input_dir, 
            output_path, 
            compression_level, 
            max_tokens_per_chapter,
            enable_polish
        )
    except Exception as e:
        print(f"âŒ é”™è¯¯: å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())